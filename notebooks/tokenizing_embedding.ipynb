{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d87c96",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6e7f409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2618 total XML files. Filtering by whitelist...\n",
      "Extracted 169 texts from the whitelist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from lxml import etree\n",
    "\n",
    "whitelist_authors = [\n",
    "    \"tlg0543\", # Polybius\n",
    "    \"tlg0527\", # Septuagint\n",
    "    \"tlg0007\", # Plutarch\n",
    "    \"tlg0557\", # Epictetus\n",
    "    \"tlg0062\", # Lucian\n",
    "    \"tlg0525\", # Pausanius\n",
    "    \"tlg0057\", # Galen\n",
    "    \"tlg0551\", # Appian\n",
    "    \"tlg0526\", # Josephus\n",
    "]\n",
    "\n",
    "def extract_clean_text(repo_path):\n",
    "    # 1. Expand the ~ to /home/user or /Users/user\n",
    "    repo_path = os.path.expanduser(repo_path)\n",
    "    all_text = []\n",
    "    \n",
    "    # 2. Use a recursive glob to find all xml files\n",
    "    search_pattern = os.path.join(repo_path, \"data\", \"**\", \"*.xml\")\n",
    "    files = glob.glob(search_pattern, recursive=True)\n",
    "    \n",
    "    print(f\"Found {len(files)} total XML files. Filtering by whitelist...\")\n",
    "\n",
    "    for xml_file in files:\n",
    "        # 3. Get the folder name that contains the work (usually the author ID)\n",
    "        # First1KGreek structure: data/tlg0007/tlg001/file.xml\n",
    "        parts = xml_file.split(os.sep)\n",
    "        \n",
    "        # We look for the folder immediately following 'data'\n",
    "        if 'data' in parts:\n",
    "            data_index = parts.index('data')\n",
    "            author_id = parts[data_index + 1]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if author_id in whitelist_authors:\n",
    "            try:\n",
    "                tree = etree.parse(xml_file)\n",
    "                root = tree.getroot()\n",
    "                # Use a wildcard namespace if the URI varies\n",
    "                # and target the <body> to avoid metadata/header text\n",
    "                text_content = \" \".join(root.xpath(\"//*[local-name()='body']//text()\"))\n",
    "                \n",
    "                # Basic cleaning: remove extra whitespace\n",
    "                clean_str = \" \".join(text_content.split())\n",
    "                if clean_str:\n",
    "                    all_text.append(clean_str)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {xml_file}: {e}\")\n",
    "                \n",
    "    return all_text\n",
    "\n",
    "path_to_greek_repo = \"~/Documents/codespace/projects/First1KGreek\"\n",
    "all_text = extract_clean_text(path_to_greek_repo)\n",
    "print(f\"Extracted {len(all_text)} texts from the whitelist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2077a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/combined_text_NT.txt', 'r', encoding='utf-8') as f:\n",
    "    nt_greek = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64f56b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text.append(nt_greek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e99d1498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 197976\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_into_sentences(text_list):\n",
    "    sentences = []\n",
    "    # This regex looks for Greek punctuation: . ; (high dot) and ?\n",
    "    # It also handles the Greek question mark (;) and high dot (·)\n",
    "    terminal_punctuation = re.compile(r'([.;·?])\\s+')\n",
    "    \n",
    "    for text in text_list:\n",
    "        # Split by punctuation and keep the punctuation\n",
    "        chunks = terminal_punctuation.split(text)\n",
    "        # Re-join the punctuation with the sentence\n",
    "        for i in range(0, len(chunks)-1, 2):\n",
    "            sent = chunks[i] + chunks[i+1]\n",
    "            if len(sent.strip()) > 5: # Ignore tiny fragments\n",
    "                sentences.append(sent.strip())\n",
    "    return sentences\n",
    "\n",
    "# Process your list\n",
    "all_sentences = split_into_sentences(all_text)\n",
    "print(f\"Total sentences: {len(all_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d964c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/greek_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\\n\".join(all_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4302769d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23377805"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\\n\".join(all_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8c925",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2e07f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3f2b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c7dd258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./vocab.json', './merges.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_path = 'data/greek_corpus.txt'\n",
    "\n",
    "tokenizer.train(\n",
    "    files=[corpus_path], \n",
    "    vocab_size=30_000, \n",
    "    min_frequency=2, \n",
    "    show_progress=True,\n",
    "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    ")\n",
    "tokenizer.save_model('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ec5805e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['á¼ĲÎ½', 'Ġá¼ĢÏģÏĩá¿ĩ', 'Ġá¼¦Î½', 'Ġá½ģ', 'ĠÎ»ÏĮÎ³Î¿ÏĤ']\n",
      "IDs: [994, 3797, 946, 398, 4121]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"ἐν ἀρχῇ ἦν ὁ λόγος\"\n",
    "encoded = tokenizer.encode(test_sentence)\n",
    "\n",
    "print(f\"Tokens: {encoded.tokens}\")\n",
    "print(f\"IDs: {encoded.ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a1bde5",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d67eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaForMaskedLM\n",
    "\n",
    "# Configure the architecture\n",
    "config = RobertaConfig(\n",
    "    vocab_size=30000,       # Must match your tokenizer vocab size\n",
    "    max_position_embeddings=514, \n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,    # Half of BERT-base for speed/efficiency\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "# Initialize the model with random weights\n",
    "# This is a \"Fresh\" model - it knows nothing yet!\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a98fdce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Try the slow tokenizer first to debug\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\n",
    "    \".\",\n",
    "    max_len=512,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    sep_token=\"</s>\",\n",
    "    cls_token=\"<s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\" # Fixed the semicolon here\n",
    ")\n",
    "\n",
    "# If that works, save it to generate the proper 'tokenizer.json' \n",
    "# which the Fast version prefers\n",
    "tokenizer.save_pretrained(\"./fixed_tokenizer\")\n",
    "\n",
    "# Now load the Fast version\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer_fast = RobertaTokenizerFast.from_pretrained(\"./fixed_tokenizer\")\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=True, \n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f009e38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 197977 examples [00:00, 582986.55 examples/s]\n",
      "Map: 100%|██████████| 197977/197977 [00:06<00:00, 31489.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Load the raw text file\n",
    "# This treats each line in your txt file as one 'row' in the dataset\n",
    "raw_dataset = load_dataset(\"text\", data_files={\"train\": \"data/greek_corpus.txt\"})\n",
    "\n",
    "# 2. Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    # This handles the padding and truncation for each line\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        max_length=128, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "# map() runs the tokenization in parallel across your CPU cores\n",
    "tokenized_datasets = raw_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=[\"text\"],\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "\n",
    "# 3. Get the train split\n",
    "train_dataset = tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "464378fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? True\n",
      "Device Name: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
    "print(f\"Device Name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daa6fdce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30935' max='30935' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30935/30935 1:05:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>8.166827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>7.522499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>7.326800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>7.117297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>6.983081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>6.883788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>6.763104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>6.697385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>6.590288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>6.510223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>6.403869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>6.307435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>6.198952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>6.101020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>6.009817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>5.941376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>5.843373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>5.784558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>5.699076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>5.617067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>5.587505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>5.542240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>5.481683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>5.450229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>5.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>5.310163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>5.272481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>5.241515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>5.245498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>5.205773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>5.154595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>5.125563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>5.089377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>5.069830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>5.047268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>5.036219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>5.012793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>4.947099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>4.937062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>4.926456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>4.922523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>4.886225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>4.883762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>4.840082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>4.813688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>4.852281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>4.805479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>4.801640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>4.770153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>4.776009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>4.738285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>4.744776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>4.739622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>4.711240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>4.740435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>4.706248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>4.710525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>4.707139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>4.692540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>4.670664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>4.695306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30935, training_loss=5.475901098699895, metrics={'train_runtime': 3952.0835, 'train_samples_per_second': 250.472, 'train_steps_per_second': 7.828, 'total_flos': 3.280467815958528e+16, 'train_loss': 5.475901098699895, 'epoch': 5.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./GreekBERT\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32, # Increase this if you have high VRAM (e.g., 12GB+)\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    \n",
    "    # GPU Optimizations\n",
    "    fp16=True,                # Uses half-precision for massive speedup\n",
    "    dataloader_num_workers=4, # Uses multiple CPU cores to feed the GPU\n",
    "    report_to=\"none\",         # Keeps things simple, no external logging\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator, # This still uses the mask_token logic from before\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce4d8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ././GreekBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Save the model weights and configuration\n",
    "trainer.save_model(\"./GreekBERT\")\n",
    "\n",
    "# Save the tokenizer specifically into the same folder\n",
    "tokenizer.save_pretrained(\"./GreekBERT\")\n",
    "\n",
    "print(\"Model saved to ././GreekBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5be1212e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Check this in your trainer output\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal steps expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtrainer\u001b[49m.state.max_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Check this in your trainer output\n",
    "print(f\"Total steps expected: {trainer.state.max_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf3dfc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in list: 197976\n",
      "Number of rows in train_dataset: 197977\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of sentences in list: {len(all_sentences)}\")\n",
    "print(f\"Number of rows in train_dataset: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b57ad0a",
   "metadata": {},
   "source": [
    "The loss is pretty high. Let's see if we can add more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c64f2ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Greek word predictions (ignoring punctuation):\n",
      "Score: 0.0072 | Token: ὼν\n",
      "Score: 0.0056 | Token: ὸς\n",
      "Score: 0.0044 | Token: ὴς\n",
      "Score: 0.0043 | Token: οῦσιν\n"
     ]
    }
   ],
   "source": [
    "results = fill_mask(\"ἐν <mask> ἦν ὁ λόγος\")\n",
    "\n",
    "print(\"Top 5 Greek word predictions (ignoring punctuation):\")\n",
    "count = 0\n",
    "for res in results:\n",
    "    token = res['token_str'].strip()\n",
    "    # Skip common punctuation marks\n",
    "    if token in [\".\", \"·\", \";\", \",\", \"»\", \"«\", \"’\", \"΄\", \"·\", '\"']:\n",
    "        continue\n",
    "    print(f\"Score: {res['score']:.4f} | Token: {token}\")\n",
    "    count += 1\n",
    "    if count == 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34a9ed12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (God/Lord): 0.8566\n",
      "Similarity (God/Sword): 0.8424\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Helper to get the average embedding (Mean Pooling)\n",
    "def get_embedding(text, model, tokenizer):\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.roberta(**inputs) # Use .roberta to get hidden states\n",
    "    # Mean across the token dimension\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Compare two words\n",
    "emb1 = get_embedding(\"θεός\", model, tokenizer)\n",
    "emb2 = get_embedding(\"κύριος\", model, tokenizer)\n",
    "emb3 = get_embedding(\"μάχαιρα\", model, tokenizer)\n",
    "\n",
    "cos_sim_holy = util.cos_sim(emb1, emb2)\n",
    "cos_sim_war = util.cos_sim(emb1, emb3)\n",
    "\n",
    "print(f\"Similarity (God/Lord): {cos_sim_holy.item():.4f}\")\n",
    "print(f\"Similarity (God/Sword): {cos_sim_war.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a37082a",
   "metadata": {},
   "source": [
    "# Iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90027e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jon/Documents/codespace/projects/semantic_domain_for_biblical_greek/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import (\n",
    "    RobertaConfig, RobertaForMaskedLM, RobertaTokenizer, \n",
    "    RobertaTokenizerFast, DataCollatorForLanguageModeling, \n",
    "    TrainingArguments, Trainer, pipeline\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import util\n",
    "\n",
    "def run_greek_experiment(experiment_name, corpus_path, num_epochs=5, vocab_size=30000):\n",
    "    print(f\"--- Starting Experiment: {experiment_name} ---\")\n",
    "    \n",
    "    # 1. Setup Architecture\n",
    "    config = RobertaConfig(\n",
    "        vocab_size=vocab_size,\n",
    "        max_position_embeddings=514,\n",
    "        num_attention_heads=12,\n",
    "        num_hidden_layers=6,\n",
    "        type_vocab_size=1,\n",
    "    )\n",
    "    model = RobertaForMaskedLM(config=config)\n",
    "    \n",
    "    # 2. Load Tokenizer (from your local files)\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\".\", max_len=512)\n",
    "    \n",
    "    # 3. Prepare Dataset\n",
    "    raw_dataset = load_dataset(\"text\", data_files={\"train\": corpus_path})\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, max_length=128, padding=\"max_length\")\n",
    "    \n",
    "    tokenized_datasets = raw_dataset.map(\n",
    "        tokenize_function, batched=True, remove_columns=[\"text\"], load_from_cache_file=False\n",
    "    )\n",
    "    \n",
    "    # 4. Collator & Training Args\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./{experiment_name}\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=32,\n",
    "        save_total_limit=1,\n",
    "        prediction_loss_only=True,\n",
    "        fp16=True,\n",
    "        dataloader_num_workers=4,\n",
    "        report_to=\"none\",\n",
    "        logging_steps=500\n",
    "    )\n",
    "    \n",
    "    # 5. Train\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model, args=training_args, \n",
    "        data_collator=data_collator, train_dataset=tokenized_datasets[\"train\"]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # 6. Save\n",
    "    model_path = f\"./models/{experiment_name}\"\n",
    "    trainer.save_model(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    \n",
    "    return model, tokenizer, model_path\n",
    "\n",
    "# --- UTILITY FOR TESTING ---\n",
    "def test_semantic_domains(model, tokenizer):\n",
    "    model.eval()\n",
    "    def get_emb(text):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model.roberta(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    god = get_emb(\"θεός\")\n",
    "    lord = get_emb(\"κύριος\")\n",
    "    sword = get_emb(\"μάχαιρα\")\n",
    "    \n",
    "    print(f\"Similarity (God/Lord): {util.cos_sim(god, lord).item():.4f}\")\n",
    "    print(f\"Similarity (God/Sword): {util.cos_sim(god, sword).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc2701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Experiment: Greek_v1_Base ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 395954 examples [00:06, 29966.35 examples/s]          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30935' max='30935' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30935/30935 1:09:42, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>8.189638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>7.539497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>7.333592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>7.121885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>6.987069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>6.882024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>6.771735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>6.708227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>6.600252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>6.531779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>6.426525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>6.324432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>6.216148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>6.107960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>6.023638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>5.943942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>5.847725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>5.787828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>5.703812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>5.622479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>5.585470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>5.538865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>5.479724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>5.445299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>5.373491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>5.307320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>5.271681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>5.238917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>5.237673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>5.203424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>5.153706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>5.119275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>5.084379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>5.070665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>5.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>5.035611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>5.010666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>4.942173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>4.933825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>4.927083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>4.918543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>4.888528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>4.881341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>4.838605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>4.820518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>4.855808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>4.811706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>4.805425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>4.771737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>4.777844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>4.742287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>4.746879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>4.742333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>4.716825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>4.742406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>4.708529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>4.716890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>4.711411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>4.698800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>4.670181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>4.698575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.47it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.36it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.58it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.38it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.36it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.39it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.38it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.53it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.53it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.32it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.53it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.39it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.39it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.39it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.38it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (God/Lord): 0.6471\n",
      "Similarity (God/Sword): 0.7097\n",
      "--- Starting Experiment: Greek_v2_Expanded ---\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find '/home/jon/Documents/codespace/projects/semantic_domain_for_biblical_greek/data/greek_corpus_large.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m test_semantic_domains(model_v1, tok_v1)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Experiment 2: More data + more epochs\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Let's say you saved a bigger file called 'data/greek_corpus_large.txt'\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model_v2, tok_v2, path_v2 = \u001b[43mrun_greek_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGreek_v2_Expanded\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/greek_corpus_large.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m test_semantic_domains(model_v2, tok_v2)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mrun_greek_experiment\u001b[39m\u001b[34m(experiment_name, corpus_path, num_epochs, vocab_size)\u001b[39m\n\u001b[32m     25\u001b[39m tokenizer = RobertaTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, max_len=\u001b[32m512\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 3. Prepare Dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m raw_dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_path\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize_function\u001b[39m(examples):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m], truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=\u001b[32m128\u001b[39m, padding=\u001b[33m\"\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codespace/projects/semantic_domain_for_biblical_greek/.venv/lib/python3.12/site-packages/datasets/load.py:1488\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1483\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   1484\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   1485\u001b[39m )\n\u001b[32m   1487\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1488\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codespace/projects/semantic_domain_for_biblical_greek/.venv/lib/python3.12/site-packages/datasets/load.py:1133\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1132\u001b[39m     features = _fix_for_backward_compatible_features(features)\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[32m   1143\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codespace/projects/semantic_domain_for_biblical_greek/.venv/lib/python3.12/site-packages/datasets/load.py:915\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;66;03m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    894\u001b[39m \u001b[38;5;66;03m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    906\u001b[39m \n\u001b[32m    907\u001b[39m \u001b[38;5;66;03m# Try packaged\u001b[39;00m\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[32m    909\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPackagedDatasetModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[38;5;66;03m# Try locally\u001b[39;00m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m path.endswith(filename):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codespace/projects/semantic_domain_for_biblical_greek/.venv/lib/python3.12/site-packages/datasets/load.py:528\u001b[39m, in \u001b[36mPackagedDatasetModuleFactory.get_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    522\u001b[39m base_path = Path(\u001b[38;5;28mself\u001b[39m.data_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).expanduser().resolve().as_posix()\n\u001b[32m    523\u001b[39m patterns = (\n\u001b[32m    524\u001b[39m     sanitize_patterns(\u001b[38;5;28mself\u001b[39m.data_files)\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    526\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns(base_path, download_config=\u001b[38;5;28mself\u001b[39m.download_config)\n\u001b[32m    527\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m data_files = \u001b[43mDataFilesDict\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m module_path, \u001b[38;5;28mhash\u001b[39m = _PACKAGED_DATASETS_MODULES[\u001b[38;5;28mself\u001b[39m.name]\n\u001b[32m    536\u001b[39m builder_kwargs = {\n\u001b[32m    537\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdata_files\u001b[39m\u001b[33m\"\u001b[39m: data_files,\n\u001b[32m    538\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdataset_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m    539\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codespace/projects/semantic_domain_for_biblical_greek/.venv/lib/python3.12/site-packages/datasets/data_files.py:708\u001b[39m, in \u001b[36mDataFilesDict.from_patterns\u001b[39m\u001b[34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[39m\n\u001b[32m    703\u001b[39m out = \u001b[38;5;28mcls\u001b[39m()\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns.items():\n\u001b[32m    705\u001b[39m     out[key] = (\n\u001b[32m    706\u001b[39m         patterns_for_key\n\u001b[32m    707\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mDataFilesList\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    714\u001b[39m     )\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codespace/projects/semantic_domain_for_biblical_greek/.venv/lib/python3.12/site-packages/datasets/data_files.py:601\u001b[39m, in \u001b[36mDataFilesList.from_patterns\u001b[39m\u001b[34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[39m\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[32m    599\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    600\u001b[39m         data_files.extend(\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m             \u001b[43mresolve_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m                \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m                \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m         )\n\u001b[32m    608\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m    609\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codespace/projects/semantic_domain_for_biblical_greek/.venv/lib/python3.12/site-packages/datasets/data_files.py:390\u001b[39m, in \u001b[36mresolve_pattern\u001b[39m\u001b[34m(pattern, base_path, allowed_extensions, download_config)\u001b[39m\n\u001b[32m    388\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    389\u001b[39m         error_msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m with any supported extension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(allowed_extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Unable to find '/home/jon/Documents/codespace/projects/semantic_domain_for_biblical_greek/data/greek_corpus_large.txt'"
     ]
    }
   ],
   "source": [
    "# Experiment 1: The original data\n",
    "model_v1, tok_v1, path_v1 = run_greek_experiment(\"Greek_v1_Base\", \"data/greek_corpus.txt\", num_epochs=5)\n",
    "test_semantic_domains(model_v1, tok_v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb45cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Experiment: Greek_v2_Expanded ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 395954 examples [00:06, 28966.81 examples/s]          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='92805' max='92805' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [92805/92805 3:26:37, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>8.178635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>7.532491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>7.326483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>7.111935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>6.973551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>6.873160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>6.751871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>6.683003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>6.566755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>6.486512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>6.356966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>6.238390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>6.103858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>5.986416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>5.882622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>5.801836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>5.701292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>5.635620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>5.554859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>5.469896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>5.438162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>5.387146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>5.328425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>5.292661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>5.214624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>5.150151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>5.109423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>5.071190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>5.072638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>5.033056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>4.974683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>4.947596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>4.897932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>4.877095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>4.850753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>4.833074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>4.803759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>4.719386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>4.705887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>4.684757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>4.679486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>4.633691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>4.620449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>4.568405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>4.540108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>4.562479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>4.515968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>4.498969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>4.452803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>4.442339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>4.393918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>4.389609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>4.369225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>4.340338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>4.349254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>4.309385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>4.303837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>4.286549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>4.271110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>4.223158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>4.240752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>4.195971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>4.165414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>4.151710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>4.172480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>4.130711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>4.119320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>4.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>4.075774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>4.073816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>4.053656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>4.040953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>4.035376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>4.053591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>4.012195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>3.974230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>3.953154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>3.941069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>3.949322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>3.921781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>3.947474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>3.908329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>3.920925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>3.904752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>3.904664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>3.892495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>3.842111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>3.802946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>3.843233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>3.829355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>3.804572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>3.815806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>3.795319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>3.780004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>3.791428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>3.762596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>3.786759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>3.765032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>3.776633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>3.723407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>3.716123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>3.704787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>3.712630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>3.694979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>3.695284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>3.686986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>3.686893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>3.663808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>3.647624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>3.699839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>3.635729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>3.628384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>3.610802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>3.633221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>3.606260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>3.611375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>3.588602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>3.586292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>3.584643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>3.575159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>3.595840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>3.581364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>3.570386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>3.568640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>3.534446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>3.551858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>3.540074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>3.509291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>3.532611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>3.533356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>3.525854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>3.512732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>3.507422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>3.503470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>3.508516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>3.499139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>3.497286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>3.468602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>3.474119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>3.466664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>3.459218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>3.472323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>3.439751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>3.456180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>3.448268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>3.431163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>3.450110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>3.421483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>3.441036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>3.411919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75500</td>\n",
       "      <td>3.413365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>3.380850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>3.411561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>3.415076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>3.394032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>3.401546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>3.419181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>3.406547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79500</td>\n",
       "      <td>3.401173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>3.375182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80500</td>\n",
       "      <td>3.382411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>3.359239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81500</td>\n",
       "      <td>3.391407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>3.365931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82500</td>\n",
       "      <td>3.379951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>3.375833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83500</td>\n",
       "      <td>3.378625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>3.370970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84500</td>\n",
       "      <td>3.345582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>3.337352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85500</td>\n",
       "      <td>3.361324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>3.356427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86500</td>\n",
       "      <td>3.355348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>3.333031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87500</td>\n",
       "      <td>3.325945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>3.338073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88500</td>\n",
       "      <td>3.375657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>3.350226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89500</td>\n",
       "      <td>3.329399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>3.341400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90500</td>\n",
       "      <td>3.349385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>3.336760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91500</td>\n",
       "      <td>3.311557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>3.340825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92500</td>\n",
       "      <td>3.339448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.69it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.53it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.58it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.81it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.56it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.69it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.69it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (God/Lord): 0.6973\n",
      "Similarity (God/Sword): 0.6839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Experiment 2: More data + more epochs\n",
    "# Let's say you saved a bigger file called 'data/greek_corpus_large.txt'\n",
    "model_v2, tok_v2, path_v2 = run_greek_experiment(\"Greek_v2_Expanded\", \"data/greek_corpus.txt\", num_epochs=15)\n",
    "test_semantic_domains(model_v2, tok_v2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-domain-for-biblical-greek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
